#+MACRO: otb この設定値はトピック毎に上書き可能です( [[#topic-config][トピックレベルの設定]] を参照)。
* =broker.id=
| デフォルト |   |

各ブローカは非負整数の ID により一意に識別されます。
この ID はブローカの「名前」として使われ、
そのブローカが異なるホスト/ポートに移動した際にもコンシューマは混乱なく利用出来るようになります。

クラスタ内でユニークでありさえすれば任意の数値を設定できます。

* =log.dirs=
| デフォルト | =/tmp/kafka-logs= |

Kafka のデータが保存される1つ以上のディレクトリです。
複数指定する際はカンマで区切って指定します。
新しく作られた各パーティションは、
その時点で保持しているパーティションが最も少ないディレクトリに配置されます。

* =port=
| デフォルト | =9092= |

クライアントからの接続を受け付けるサーバのポート番号です。

* =zookeeper.connect=
| デフォルト | =null= |

ZooKeeperとの接続情報を =hostname:port= という形式で文字列で指定します。
=hostname= 、 =port= はそれぞれ ZooKeeper クラスタに属するノードのホスト名とポート番号です。
=hostname1:port1,hostname2:port2,hostname3:port3= のように複数のホストを指定することで、
ホストダウン時に他のZooKeeperノードへ接続出来るように出来ます。

ZooKeeper では "chroot" パスを追加することも出来、
これによってクラスタ内の全ての Kafka データが特定のパス以下に配置されるように設定出来ます。
こうすることで、異なる Kafka クラスタや他のアプリケーションを、同じ ZooKeeper クラスタ上に構築出来ます。
具体的には、 =hostname1:port1,hostname2:port2,hostname3:port3/chroot/path= のように指定することで、
全 Kafka クラスタのデータが =/chroot/path= 以下に配置されるように出来ます。

#+begin_note
chroot を指定する場合、ブローカを起動する前にパスを作っておく必要があります。
また、全コンシューマが同じ接続情報文字列を利用するようにしなければなりません。
#+end_note

* =message.max.bytes=
:PROPERTIES:
:CUSTOM_ID: borker-message-max-bytes
:END:

| デフォルト | 1000000 |

サーバが受け取るメッセージの最大サイズです。
このプロパティは [[#consumer-fetch-message-max-bytes][コンシューマが利用する最大取得サイズ設定]] と同調して設定するよう注意しましょう。
さもなければ、乱暴なプロデューサがコンシューマが扱えない程のサイズのメッセージを
パブリッシュすることが出来るようになってしまいます。


* =num.network.threads=
| デフォルト | 3 |

サーバがネットワークリクエストを処理するのに使うネットワークスレッドの数です。
恐らく変更する必要はありません。

* =num.io.threads=
| デフォルト | 8 |

サーバがリクエストを処理するために使う I/O スレッドの数です。
少なくとも利用するディスク数分は用意するべきです。

* =background.threads=
| デフォルト | 10 |

ファイル削除のような様々なバックグラウンド処理を行なう為に使われるスレッドの数です。
変更する必要はないでしょう。

* =queued.max.requests=
| デフォルト | 500 |

I/O スレッドが処理する為にキューに詰まれる最大リクエスト数で、
この数までキューに詰まれると、ネットワークスレッドは新規リクエストを読むのを止めます。

* =host.name=
| デフォルト | null |

ブローカのホスト名です。
この値が設定されていれば、そのアドレスにだけバインドします。
設定されていなければ、全インタフェースにバインドします。
この情報は ZooKeeper にパブリッシュされクライアントから利用されます。

* =advertised.host.name=
| デフォルト | null |

この値が設定されていれば、 プロデューサやコンシューマ、そして他のブローカが接続するホスト名として利用されます。

[fn:: (訳注) 関連 issue [[https://issues.apache.org/jira/browse/KAFKA-1092][KAFKA-1092]]]

* =advertised.port=
| デフォルト | null |

プロデューサやコンシューマ、そして他のブローカが接続するポート番号です。
サーバがバインドするポートと異なる場合のみ必要な設定です。

* =socket.send.buffer.bytes=
| デフォルト | 100 * 1024 |

ソケット接続時にサーバが利用する =SO_SNDBUFF= バッファの値です。

* =socket.receive.buffer.bytes=
| デフォルト | 100 * 1024 |

ソケット接続時にサーバが利用する =SO_RCVBUFF= バッファの値です。

* =socket.request.max.bytes=
| デフォルト | 100 * 1024 * 1024 |

サーバが許容する最大リクエストサイズです。
メモリ不足に陥らないよう、 Java ヒープサイズよりも小さい値に設定すべきです。

* =num.partitions=
| デフォルト | 1 |

トピック作成時に指定されなかった場合のデフォルトパーティション数です。


* =log.segment.bytes=
| デフォルト | 1024 * 1024 * 1024 |

トピックパーティションのログは、セグメントファイルのディレクトリとして保存されています。
1セグメントのファイルサイズがこの値に達すると、新しいセグメントファイルが作成されます。
[fn:: (訳注) =log_dirs= に、 =<トピック名>-<パーティション番号>/000...000.log= の様に保存されています。 ]
{{{otb}}}

* =log.roll.{ms,hours}=
| デフォルト | 24 * 7 hours |

セグメントファイルサイズが =log.segment.bytes= に到達していない場合でも、
この設定値の時間が経過した場合に強制的に新たなログセグメントを作成するよう設定します。
{{{otb}}}

* =log.cleanup.policy=
| デフォルト | delete |

=delete= または =compact= を設定出来ます。
ログセグメントがサイズや時間の上限に達した際に、
=delete= の場合は削除され、 =compact= の場合は [[http://kafka.apache.org/documentation.html#compaction][ログコンパクション]] が行なわれます。
{{{otb}}}

* =log.retention.{ms,minutes,hours}=
| デフォルト | 7 days |

ログセグメントを削除するまでの時間、つまりトピックのデフォルト保持期間です。

#+begin_note
=log.retention.minutes= と =log.retention.bytes= が両方セットされていた場合、
いずれかの上限に達した時点でクリーンアップを行ないます。
#+end_note

{{{otb}}}

* =log.retention.bytes=
| デフォルト | -1 |

各トピックパーティションログの総サイズ制限です。
これはパーティション毎の制限なので、トピックが必要とするトータルのデータ容量は、これにパーティション数を掛けた値になります。

#+begin_note
=log.retention.minutes= と =log.retention.bytes= が両方セットされていた場合、
いずれかの上限に達した時点でクリーンアップを行ないます。
#+end_note

{{{otb}}}

* =log.retention.check.interval.ms=
| デフォルト | 5 minutes |

ログの保持ポリシーと照らし合わせて、削除対象となるログセグメントがあるかどうかを確認する間隔です。

* =log.cleaner.enable=
| デフォルト | false |

ログコンパクションを実行するためには、この設定は必ず =true= にしなければなりません。

* =log.cleaner.threads=
| デフォルト | 1 |

ログコンパクション実行時のログクリーニングに使われるスレッド数です。

* =log.cleaner.io.max.bytes.per.second=
| デフォルト | Double.MaxValue |

ログクリーナがログコンパクション実行時に発生する I/O の最大総サイズです。
この制限を設けることで、クリーナが運用中のサービスに影響を与えることを避けることが出来ます。

* =log.cleaner.dedupe.buffer.size=
| デフォルト | 500*1024*1024 |

ログクリーナがクリーニング中にインデクシングとログの重複除去のために使用するバッファサイズです。
十分なメモリがあるならば、より大きな値の方が望ましいです。

* =log.cleaner.io.buffer.size=
| デフォルト | 512*1024 |

ログクリーニング中に使用される I/O チャンクのサイズです。
恐らく変更する必要はないでしょう。

* =log.cleaner.io.buffer.load.factor=
| デフォルト | 0.9 |

ログクリーニング中に使用されるハッシュテーブルの load factor です。
恐らく変更する必要はないでしょう。

* =log.cleaner.backoff.ms=
| デフォルト | 15000 |

クリーニングが必要なログが無いか確認する間隔です。

* =log.cleaner.min.cleanable.ratio=
| デフォルト | 0.5 |

[[http://kafka.apache.org/documentation.html#compaction][ログコンパクション]] が有効なときの、ログコンパクタがログをクリーンする頻度を設定します。
デフォルトでは50%以上のログがコンパクションされていた場合はクリーニングを行ないません。
この比率はログの重複により無駄に使用される最大スペースを設定します
(50%だと、多くて50%のログが重複している可能性がある、ということです)。
より高い比率に設定すれば、少ない回数で、より効率的なクリーニングが行われることになりますが、
それは同時にログが無駄に使用するスペースがより多くなるということにもなります。
{{{otb}}}

* =log.cleaner.delete.retention.ms=
| デフォルト | 1 day |

[[http://kafka.apache.org/documentation.html#compaction][ログコンパクション]] されたトピックの削除トゥームストーンマーカを保持する期間です。
最終段階の有効なスナップショットを確実に得る為にオフセット 0 から読み込みを開始する場合、
コンシューマはこの期間内に読み込みを完了させる必要があります。
{{{otb}}}

* =log.index.size.max.bytes=
| デフォルト | 10 * 1024 * 1024 |

各ログセグメントのオフセットインデックスが使用する容量の最大バイト数です。
ここで設定した容量のスパースファイルを事前に確保して、
新たなログセグメント作成のタイミングで実容量まで縮める、という動作をする点に注意してください。
インデックスがいっぱいになってしまった場合は、
=log.segment.bytes= を超過していない場合でも新たなログセグメントを作成します。
{{{otb}}}

* =log.index.interval.bytes=
| デフォルト | 4096 |

オフセットインデックスにエントリを追加するバイト間隔です。
取得リクエストを実行する際、サーバは取得を開始、あるいは終了するために、
正しいログ内の位置を見つけるため、ここで設定したバイト数まで線形走査する必要があります。
そのためこの値を大きくすればする程インデックスファイルは大きくなり
(そしてもう少しだけメモリを使用するようになり)ますが、走査回数は少なくなります。
[fn:: (訳注)何言ってんのかよく分かんない]
ただ、サーバはログ追加毎に2つ以上インデックスエントリを追加することは決してありません
(たとえ =log.index.interval= を上回るメッセージが追加されたとしても)。
普通はこの値をいじる必要は無いでしょう。

* =log.flush.interval.messages=
| デフォルト | Long.MaxValue |

ここで設定された数までメッセージをログパーティションに書き込んだら、強制的にログを fsync します。
この値を小さくすれば頻繁にディスクにデータを同期するようになりますが、パフォーマンスに多大な影響を与えます。
耐久性を得るためには、単一サーバの fsync に依存するよりもレプリケーションを利用することを通常は推奨しますが、
さらなる確実性を求める場合はこの設定を利用することも出来ます。

* =log.flush.scheduler.interval.ms=
| デフォルト | Long.MaxValue |
ログフラッシャがディスクにフラッシュするのに適したログがあるかをチェックする頻度をミリ秒で設定します.

* =log.flush.interval.ms=
| デフォルト | Long.MaxValue |

ログの fsync がコールされるまでの最大時間です。

=log.flush.interval.messages= と合わせて設定された場合は、
どちらかの条件を満たした時点でログがフラッシュされます。

* =log.delete.delay.ms=
| デフォルト | 60000 |

メモリ上のセグメントインデックスから除去された後にログファイルを保持しておく期間です。
この期間はロックせずとも進行中の読み込みを中断することなく完了することが出来ます。
[fn:: 何言ってんのかよく分かんない]
通常はこの値を変更する必要は無いでしょう。

* =log.flush.offset.checkpoint.interval.ms=
| デフォルト | 60000 |

リカバリのためにログの最終フラッシュのチェックポイントをセットする頻度です。
これは変更すべきではありません。

* =log.segment.delete.delay.ms=
| デフォルト | 60000 |

ファイルシステムからログセグメントファイルを削除するまでの時間です。

* =auto.create.topics.enable=
| デフォルト | true |

サーバに自動でトピックを作成可能にします。
この設定が有効な場合、存在しないトピックを作成、あるいはそのメタデータを取得しようとした際に、
デフォルトのレプリケーションファクタとパーティション数で自動的にトピックが作成されます。

* =controller.socket.timeout.ms=
| デフォルト | 30000 |

レプリカに対するパーティション管理コントローラのソケットタイムアウトです。

* =controller.message.queue.size=
| デフォルト | Int.MaxValue |

コントローラからブローカへの接続チャネル( =controller-to-broker-channels= )のバッファサイズです。

* =default.replication.factor=
| デフォルト | 1 |
自動作成されたトピックのデフォルトレプリケーションファクタです。

* =replica.lag.time.max.ms=
| デフォルト | 10000 |

フォロワがここで設定した時間内に取得リクエストを送信しなかった場合、
リーダはそのフォロワを ISR (in-sync replicas) から除去し、死んだものとして扱います。

* =replica.lag.max.messages=
| デフォルト | 4000 |

ここで設定したメッセージ数よりレプリケーションが遅れた場合、
リーダはそのフォロワを ISR (in-sync replicas) から除去し、死んだものとして扱います。

* =replica.socket.timeout.ms=
| デフォルト | 30 * 1000 |

データ複製の為のリーダへのネットワークリクエストのソケットタイムアウトです。

* replica.socket.receive.buffer.bytes
| デフォルト |64 * 1024|
The socket receive buffer for network requests to the leader for replicating data.
* replica.fetch.max.bytes
| デフォルト |1024 * 1024|
The number of byes of messages to attempt to fetch for each partition in the fetch requests the replicas send to the leader.
* replica.fetch.wait.max.ms
| デフォルト |500|
The maximum amount of time to wait time for data to arrive on the leader in the fetch requests sent by the replicas to the leader.
* replica.fetch.min.bytes
| デフォルト |1|
Minimum bytes expected for each fetch response for the fetch requests from the replica to the leader. If not enough bytes, wait up to replica.fetch.wait.max.ms for this many bytes to arrive.
* num.replica.fetchers
| デフォルト |1|
Number of threads used to replicate messages from leaders. Increasing this value can increase the degree of I/O parallelism in the follower broker.
* replica.high.watermark.checkpoint.interval.ms
| デフォルト |5000|
The frequency with which each replica saves its high watermark to disk to handle recovery.
* fetch.purgatory.purge.interval.requests
| デフォルト |1000|
The purge interval (in number of requests) of the fetch request purgatory.
* producer.purgatory.purge.interval.requests
| デフォルト |1000|
The purge interval (in number of requests) of the producer request purgatory.
* zookeeper.session.timeout.ms
| デフォルト |6000|
ZooKeeper session timeout. If the server fails to heartbeat to ZooKeeper within this period of time it is considered dead. If you set this too low the server may be falsely considered dead; if you set it too high it may take too long to recognize a truly dead server.
* zookeeper.connection.timeout.ms
| デフォルト |6000|
The maximum amount of time that the client waits to establish a connection to zookeeper.
* zookeeper.sync.time.ms
| デフォルト |2000|
How far a ZK follower can be behind a ZK leader.
* controlled.shutdown.enable
| デフォルト |true|
Enable controlled shutdown of the broker. If enabled, the broker will move all leaders on it to some other brokers before shutting itself down. This reduces the unavailability window during shutdown.
* controlled.shutdown.max.retries
| デフォルト |3|
Number of retries to complete the controlled shutdown successfully before executing an unclean shutdown.
* controlled.shutdown.retry.backoff.ms
| デフォルト |5000|
Backoff time between shutdown retries.
* auto.leader.rebalance.enable
| デフォルト |true|
If this is enabled the controller will automatically try to balance leadership for partitions among the brokers by periodically returning leadership to the "preferred" replica for each partition if it is available.
* leader.imbalance.per.broker.percentage
| デフォルト |10|
The percentage of leader imbalance allowed per broker. The controller will rebalance leadership if this ratio goes above the configured value per broker.
* leader.imbalance.check.interval.seconds
| デフォルト |300|
The frequency with which to check for leader imbalance.
* offset.metadata.max.bytes
| デフォルト |4096|
The maximum amount of metadata to allow clients to save with their offsets.
* max.connections.per.ip
| デフォルト |Int.MaxValue|
The maximum number of connections that a broker allows from each ip address.
* max.connections.per.ip.overrides
| デフォルト ||
Per-ip or hostname overrides to the default maximum number of connections.
* connections.max.idle.ms
| デフォルト |600000|
Idle connections timeout: the server socket processor threads close the connections that idle more than this.
* log.roll.jitter.{ms,hours}
| デフォルト |0|
The maximum jitter to subtract from logRollTimeMillis.
* num.recovery.threads.per.data.dir
| デフォルト |1|
The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
* unclean.leader.election.enable
| デフォルト |true|
Indicates whether to enable replicas not in the ISR set to be elected as leader as a last resort, even though doing so may result in data loss.
* delete.topic.enable
| デフォルト |false|
Enable delete topic.
* offsets.topic.num.partitions
| デフォルト |50|
The number of partitions for the offset commit topic. Since changing this after deployment is currently unsupported, we recommend using a higher setting for production (e.g., 100-200).
* offsets.topic.retention.minutes
| デフォルト |1440|
Offsets that are older than this age will be marked for deletion. The actual purge will occur when the log cleaner compacts the offsets topic.
* offsets.retention.check.interval.ms
| デフォルト |600000|
The frequency at which the offset manager checks for stale offsets.
* offsets.topic.replication.factor
| デフォルト |3|
The replication factor for the offset commit topic. A higher setting (e.g., three or four) is recommended in order to ensure higher availability. If the offsets topic is created when fewer brokers than the replication factor then the offsets topic will be created with fewer replicas.
* offsets.topic.segment.bytes
| デフォルト |104857600|
Segment size for the offsets topic. Since it uses a compacted topic, this should be kept relatively low in order to facilitate faster log compaction and loads.
* offsets.load.buffer.size
| デフォルト |5242880|
An offset load occurs when a broker becomes the offset manager for a set of consumer groups (i.e., when it becomes a leader for an offsets topic partition). This setting corresponds to the batch size (in bytes) to use when reading from the offsets segments when loading offsets into the offset manager's cache.
* offsets.commit.required.acks
| デフォルト |-1|
The number of acknowledgements that are required before the offset commit can be accepted. This is similar to the producer's acknowledgement setting. In general, the default should not be overridden.
* offsets.commit.timeout.ms
| デフォルト |5000|
The offset commit will be delayed until this timeout or the required number of replicas have received the offset commit. This is similar to the producer request timeout.

