* =broker.id=
| デフォルト |   |

各ブローカは非負整数の ID により一意に識別されます。
この ID はブローカの「名前」として使われ、
そのブローカが異なるホスト/ポートに移動した際にもコンシューマは混乱なく利用出来るようになります。

クラスタ内でユニークでありさえすれば任意の数値を設定できます。

* =log.dirs=
| デフォルト | =/tmp/kafka-logs= |

Kafka のデータが保存される1つ以上のディレクトリです。
複数指定する際はカンマで区切って指定します。
新しく作られた各パーティションは、
その時点で保持しているパーティションが最も少ないディレクトリに配置されます。

* =port=
| デフォルト | =9092= |

クライアントからの接続を受け付けるサーバのポート番号です。

* =zookeeper.connect=
| デフォルト | =null= |

ZooKeeperとの接続情報を =hostname:port= という形式で文字列で指定します。
=hostname= 、 =port= はそれぞれ ZooKeeper クラスタに属するノードのホスト名とポート番号です。
=hostname1:port1,hostname2:port2,hostname3:port3= のように複数のホストを指定することで、
ホストダウン時に他のZooKeeperノードへ接続出来るように出来ます。

ZooKeeper では "chroot" パスを追加することも出来、
これによってクラスタ内の全ての Kafka データが特定のパス以下に配置されるように設定出来ます。
こうすることで、異なる Kafka クラスタや他のアプリケーションを、同じ ZooKeeper クラスタ上に構築出来ます。
具体的には、 =hostname1:port1,hostname2:port2,hostname3:port3/chroot/path= のように指定することで、
全 Kafka クラスタのデータが =/chroot/path= 以下に配置されるように出来ます。

#+BEGIN_NOTE
chroot を指定する場合、ブローカを起動する前にパスを作っておく必要があります。
また、全コンシューマが同じ接続情報文字列を利用するようにしなければなりません。
#+END_NOTE

* =message.max.bytes=
:PROPERTIES:
:CUSTOM_ID: borker-message-max-bytes
:END:

| デフォルト | 1000000 |

サーバが受け取るメッセージの最大サイズです。
このプロパティは [[#consumer-fetch-message-max-bytes][コンシューマが利用する最大取得サイズ設定]] と同調して設定するよう注意しましょう。
さもなければ、乱暴なプロデューサがコンシューマが扱えない程のサイズのメッセージを
パブリッシュすることが出来るようになってしまいます。


* =num.network.threads=
| デフォルト | 3 |

サーバがネットワークリクエストを処理するのに使うネットワークスレッドの数です。
恐らく変更する必要はありません。

* =num.io.threads=
| デフォルト | 8 |

サーバがリクエストを処理するために使う I/O スレッドの数です。
少なくとも利用するディスク数分は用意するべきです。

* =background.threads=
| デフォルト | 10 |

ファイル削除のような様々なバックグラウンド処理を行なう為に使われるスレッドの数です。
変更する必要はないでしょう。

* =queued.max.requests=
| デフォルト | 500 |

I/O スレッドが処理する為にキューに詰まれる最大リクエスト数で、
この数までキューに詰まれると、ネットワークスレッドは新規リクエストを読むのを止めます。

* =host.name=
| デフォルト | null |

ブローカのホスト名です。
この値が設定されていれば、そのアドレスにだけバインドします。
設定されていなければ、全インタフェースにバインドします。
この情報は ZooKeeper にパブリッシュされクライアントから利用されます。

* =advertised.host.name=
| デフォルト | null |

この値が設定されていれば、 プロデューサやコンシューマ、そして他のブローカが接続するホスト名として利用されます。

[fn:: (訳注) 関連 issue [[https://issues.apache.org/jira/browse/KAFKA-1092][KAFKA-1092]]]

* =advertised.port=
| デフォルト | null |

プロデューサやコンシューマ、そして他のブローカが接続するポート番号です。
サーバがバインドするポートと異なる場合のみ必要な設定です。

* =socket.send.buffer.bytes=
| デフォルト | 100 * 1024 |

ソケット接続時にサーバが利用する =SO_SNDBUFF= バッファの値です。

* =socket.receive.buffer.bytes=
| デフォルト | 100 * 1024 |

ソケット接続時にサーバが利用する =SO_RCVBUFF= バッファの値です。

* =socket.request.max.bytes=
| デフォルト | 100 * 1024 * 1024 |

サーバが許容する最大リクエストサイズです。
メモリ不足に陥らないよう、 Java ヒープサイズよりも小さい値に設定すべきです。

* =num.partitions=
| デフォルト | 1 |

トピック作成時に指定されなかった場合のデフォルトパーティション数です。


* =log.segment.bytes=
| デフォルト | 1024 * 1024 * 1024 |

トピックパーティションのログは、セグメントファイルのディレクトリとして保存されています。
1セグメントのファイルサイズがこの値に達すると、新しいセグメントファイルが作成されます。
[fn:: (訳注) =log_dirs= に、 =<トピック名>-<パーティション番号>/000...000.log= の様に保存されています。 ]
この設定値はトピック毎に上書き可能です( [[#topic-config][トピックレベルの設定]] を参照)。

* =log.roll.{ms,hours}=
| デフォルト | 24 * 7 hours |

セグメントファイルサイズが =log.segment.bytes= に到達していない場合でも、
この設定値の時間が経過した場合に強制的に新たなログセグメントを作成するよう設定します。
この設定値はトピック毎に上書き可能です( [[#topic-config][トピックレベルの設定]] を参照)。

* =log.cleanup.policy=
| デフォルト | delete |

=delete= または =compact= を設定出来ます。
ログセグメントがサイズや時間の上限に達した際に、
=delete= の場合は削除され、 =compact= の場合は [[http://kafka.apache.org/documentation.html#compaction][ログコンパクション]] が行なわれます。
この設定値はトピック毎に上書き可能です( [[#topic-config][トピックレベルの設定]] を参照)。

* =log.retention.{ms,minutes,hours}=
| デフォルト | 7 days |

ログセグメントを削除するまでの時間、つまりトピックのデフォルト保持期間です。

#+BEGIN_NOTE
=log.retention.minutes= と =log.retention.bytes= が両方セットされていた場合、
いずれかの上限に達した時点でクリーンアップを行ないます。
#+END_NOTE

この設定値はトピック毎に上書き可能です( [[#topic-config][トピックレベルの設定]] を参照)。

* =log.retention.bytes=
| デフォルト | -1 |

各トピックパーティションログの総サイズ制限です。
これはパーティション毎の制限なので、トピックが必要とするトータルのデータ容量は、これにパーティション数を掛けた値になります。

#+BEGIN_NOTE
=log.retention.minutes= と =log.retention.bytes= が両方セットされていた場合、
いずれかの上限に達した時点でクリーンアップを行ないます。
#+END_NOTE

この設定値はトピック毎に上書き可能です( [[#topic-config][トピックレベルの設定]] を参照)。

* =log.retention.check.interval.ms=
| デフォルト | 5 minutes |

ログの保持ポリシーと照らし合わせて、削除対象となるログセグメントがあるかどうかを確認する間隔です。

* =log.cleaner.enable=
| デフォルト | false |

ログコンパクションを実行するためには、この設定は必ず =true= にしなければなりません。

* =log.cleaner.threads=
| デフォルト | 1 |

ログコンパクション実行時のログクリーニングに使われるスレッド数です。

* =log.cleaner.io.max.bytes.per.second=
| デフォルト | Double.MaxValue |

ログクリーナがログコンパクション実行時に発生する I/O の最大総サイズです。
この制限を設けることで、クリーナが運用中のサービスに影響を与えることを避けることが出来ます。

* =log.cleaner.dedupe.buffer.size=
| デフォルト | 500*1024*1024 |

ログクリーナがクリーニング中にインデクシングとログの重複除去のために使用するバッファサイズです。
十分なメモリがあるならば、より大きな値の方が望ましいです。

* =log.cleaner.io.buffer.size=
| デフォルト | 512*1024 |

ログクリーニング中に使用される I/O チャンクのサイズです。
恐らく変更する必要はないでしょう。

* =log.cleaner.io.buffer.load.factor=
| デフォルト | 0.9 |

ログクリーニング中に使用されるハッシュテーブルの load factor です。
恐らく変更する必要はないでしょう。

* =log.cleaner.backoff.ms=
| デフォルト | 15000 |

クリーニングが必要なログが無いか確認する間隔です。

* =log.cleaner.min.cleanable.ratio=
| デフォルト | 0.5 |

[[http://kafka.apache.org/documentation.html#compaction][ログコンパクション]] が有効なときの、ログコンパクタがログをクリーンする頻度を設定します。
デフォルトでは50%以上のログがコンパクションされていた場合はクリーニングを行ないません。
この比率はログの重複により消費される最大スペースを設定します
(50%だと、多くて50%のログが重複している可能性がある、ということです)。
より高い比率に設定すれば、より少なく、効率的なクリーニングが行われることになりますが、


By default we will avoid cleaning a log where more than 50% of the log has been compacted. 
This ratio bounds the maximum space wasted in the log by duplicates (at 50% at most 50% of the log could be duplicates). 
A higher ratio will mean fewer, more efficient cleanings but will mean more wasted space in the log.

この設定値はトピック毎に上書き可能です( [[#topic-config][トピックレベルの設定]] を参照)。

* log.cleaner.delete.retention.ms
| デフォルト |1 day|
The amount of time to retain delete tombstone markers for log compacted topics. This setting also gives a bound on the time in which a consumer must complete a read if they begin from offset 0 to ensure that they get a valid snapshot of the final stage (otherwise delete tombstones may be collected before they complete their scan). This setting can be overridden on a per-topic basis (see the per-topic configuration section).
* log.index.size.max.bytes
| デフォルト |10 * 1024 * 1024|
The maximum size in bytes we allow for the offset index for each log segment. Note that we will always pre-allocate a sparse file with this much space and shrink it down when the log rolls. If the index fills up we will roll a new log segment even if we haven't reached the log.segment.bytes limit. This setting can be overridden on a per-topic basis (see the per-topic configuration section).
* log.index.interval.bytes
| デフォルト |4096|
The byte interval at which we add an entry to the offset index. When executing a fetch request the server must do a linear scan for up to this many bytes to find the correct position in the log to begin and end the fetch. So setting this value to be larger will mean larger index files (and a bit more memory usage) but less scanning. However the server will never add more than one index entry per log append (even if more than log.index.interval worth of messages are appended). In general you probably don't need to mess with this value.
* log.flush.interval.messages
| デフォルト |Long.MaxValue|
The number of messages written to a log partition before we force an fsync on the log. Setting this lower will sync data to disk more often but will have a major impact on performance. We generally recommend that people make use of replication for durability rather than depending on single-server fsync, however this setting can be used to be extra certain.
* log.flush.scheduler.interval.ms
| デフォルト |Long.MaxValue|
The frequency in ms that the log flusher checks whether any log is eligible to be flushed to disk.
* log.flush.interval.ms
| デフォルト |Long.MaxValue|
The maximum time between fsync calls on the log. If used in conjuction with log.flush.interval.messages the log will be flushed when either criteria is met.
* log.delete.delay.ms
| デフォルト |60000|
The period of time we hold log files around after they are removed from the in-memory segment index. This period of time allows any in-progress reads to complete uninterrupted without locking. You generally don't need to change this.
* log.flush.offset.checkpoint.interval.ms
| デフォルト |60000|
The frequency with which we checkpoint the last flush point for logs for recovery. You should not need to change this.
* log.segment.delete.delay.ms
| デフォルト |60000|
the amount of time to wait before deleting a file from the filesystem.
* auto.create.topics.enable
| デフォルト |true|
Enable auto creation of topic on the server. If this is set to true then attempts to produce data or fetch metadata for a non-existent topic will automatically create it with the default replication factor and number of partitions.
* controller.socket.timeout.ms
| デフォルト |30000|
The socket timeout for commands from the partition management controller to the replicas.
* controller.message.queue.size
| デフォルト |Int.MaxValue|
The buffer size for controller-to-broker-channels
* default.replication.factor
| デフォルト |1|
The default replication factor for automatically created topics.
* replica.lag.time.max.ms
| デフォルト |10000|
If a follower hasn't sent any fetch requests for this window of time, the leader will remove the follower from ISR (in-sync replicas) and treat it as dead.
* replica.lag.max.messages
| デフォルト |4000|
If a replica falls more than this many messages behind the leader, the leader will remove the follower from ISR and treat it as dead.
* replica.socket.timeout.ms
| デフォルト |30 * 1000|
The socket timeout for network requests to the leader for replicating data.
* replica.socket.receive.buffer.bytes
| デフォルト |64 * 1024|
The socket receive buffer for network requests to the leader for replicating data.
* replica.fetch.max.bytes
| デフォルト |1024 * 1024|
The number of byes of messages to attempt to fetch for each partition in the fetch requests the replicas send to the leader.
* replica.fetch.wait.max.ms
| デフォルト |500|
The maximum amount of time to wait time for data to arrive on the leader in the fetch requests sent by the replicas to the leader.
* replica.fetch.min.bytes
| デフォルト |1|
Minimum bytes expected for each fetch response for the fetch requests from the replica to the leader. If not enough bytes, wait up to replica.fetch.wait.max.ms for this many bytes to arrive.
* num.replica.fetchers
| デフォルト |1|
Number of threads used to replicate messages from leaders. Increasing this value can increase the degree of I/O parallelism in the follower broker.
* replica.high.watermark.checkpoint.interval.ms
| デフォルト |5000|
The frequency with which each replica saves its high watermark to disk to handle recovery.
* fetch.purgatory.purge.interval.requests
| デフォルト |1000|
The purge interval (in number of requests) of the fetch request purgatory.
* producer.purgatory.purge.interval.requests
| デフォルト |1000|
The purge interval (in number of requests) of the producer request purgatory.
* zookeeper.session.timeout.ms
| デフォルト |6000|
ZooKeeper session timeout. If the server fails to heartbeat to ZooKeeper within this period of time it is considered dead. If you set this too low the server may be falsely considered dead; if you set it too high it may take too long to recognize a truly dead server.
* zookeeper.connection.timeout.ms
| デフォルト |6000|
The maximum amount of time that the client waits to establish a connection to zookeeper.
* zookeeper.sync.time.ms
| デフォルト |2000|
How far a ZK follower can be behind a ZK leader.
* controlled.shutdown.enable
| デフォルト |true|
Enable controlled shutdown of the broker. If enabled, the broker will move all leaders on it to some other brokers before shutting itself down. This reduces the unavailability window during shutdown.
* controlled.shutdown.max.retries
| デフォルト |3|
Number of retries to complete the controlled shutdown successfully before executing an unclean shutdown.
* controlled.shutdown.retry.backoff.ms
| デフォルト |5000|
Backoff time between shutdown retries.
* auto.leader.rebalance.enable
| デフォルト |true|
If this is enabled the controller will automatically try to balance leadership for partitions among the brokers by periodically returning leadership to the "preferred" replica for each partition if it is available.
* leader.imbalance.per.broker.percentage
| デフォルト |10|
The percentage of leader imbalance allowed per broker. The controller will rebalance leadership if this ratio goes above the configured value per broker.
* leader.imbalance.check.interval.seconds
| デフォルト |300|
The frequency with which to check for leader imbalance.
* offset.metadata.max.bytes
| デフォルト |4096|
The maximum amount of metadata to allow clients to save with their offsets.
* max.connections.per.ip
| デフォルト |Int.MaxValue|
The maximum number of connections that a broker allows from each ip address.
* max.connections.per.ip.overrides
| デフォルト ||
Per-ip or hostname overrides to the default maximum number of connections.
* connections.max.idle.ms
| デフォルト |600000|
Idle connections timeout: the server socket processor threads close the connections that idle more than this.
* log.roll.jitter.{ms,hours}
| デフォルト |0|
The maximum jitter to subtract from logRollTimeMillis.
* num.recovery.threads.per.data.dir
| デフォルト |1|
The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
* unclean.leader.election.enable
| デフォルト |true|
Indicates whether to enable replicas not in the ISR set to be elected as leader as a last resort, even though doing so may result in data loss.
* delete.topic.enable
| デフォルト |false|
Enable delete topic.
* offsets.topic.num.partitions
| デフォルト |50|
The number of partitions for the offset commit topic. Since changing this after deployment is currently unsupported, we recommend using a higher setting for production (e.g., 100-200).
* offsets.topic.retention.minutes
| デフォルト |1440|
Offsets that are older than this age will be marked for deletion. The actual purge will occur when the log cleaner compacts the offsets topic.
* offsets.retention.check.interval.ms
| デフォルト |600000|
The frequency at which the offset manager checks for stale offsets.
* offsets.topic.replication.factor
| デフォルト |3|
The replication factor for the offset commit topic. A higher setting (e.g., three or four) is recommended in order to ensure higher availability. If the offsets topic is created when fewer brokers than the replication factor then the offsets topic will be created with fewer replicas.
* offsets.topic.segment.bytes
| デフォルト |104857600|
Segment size for the offsets topic. Since it uses a compacted topic, this should be kept relatively low in order to facilitate faster log compaction and loads.
* offsets.load.buffer.size
| デフォルト |5242880|
An offset load occurs when a broker becomes the offset manager for a set of consumer groups (i.e., when it becomes a leader for an offsets topic partition). This setting corresponds to the batch size (in bytes) to use when reading from the offsets segments when loading offsets into the offset manager's cache.
* offsets.commit.required.acks
| デフォルト |-1|
The number of acknowledgements that are required before the offset commit can be accepted. This is similar to the producer's acknowledgement setting. In general, the default should not be overridden.
* offsets.commit.timeout.ms
| デフォルト |5000|
The offset commit will be delayed until this timeout or the required number of replicas have received the offset commit. This is similar to the producer request timeout.

